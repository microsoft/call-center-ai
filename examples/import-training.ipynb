{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, install the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-ai-documentintelligence==1.0.0b1 in /Users/clemlesne/.pyenv/versions/3.11.3/lib/python3.11/site-packages (1.0.0b1)\n",
      "Requirement already satisfied: azure-search-documents==11.4.0 in /Users/clemlesne/.pyenv/versions/3.11.3/lib/python3.11/site-packages (11.4.0)\n",
      "Requirement already satisfied: unidecode==1.3.8 in /Users/clemlesne/.pyenv/versions/3.11.3/lib/python3.11/site-packages (1.3.8)\n",
      "Requirement already satisfied: nltk==3.8.1 in /Users/clemlesne/.pyenv/versions/3.11.3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.6.1 in /Users/clemlesne/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from azure-ai-documentintelligence==1.0.0b1) (0.6.1)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.28.0 in /Users/clemlesne/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from azure-ai-documentintelligence==1.0.0b1) (1.29.6)\n",
      "Requirement already satisfied: azure-common~=1.1 in /Users/clemlesne/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from azure-search-documents==11.4.0) (1.1.28)\n",
      "Requirement already satisfied: click in /Users/clemlesne/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from nltk==3.8.1) (8.1.4)\n",
      "Requirement already satisfied: joblib in /Users/clemlesne/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from nltk==3.8.1) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/clemlesne/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from nltk==3.8.1) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /Users/clemlesne/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from nltk==3.8.1) (4.65.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /Users/clemlesne/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from azure-core<2.0.0,>=1.28.0->azure-ai-documentintelligence==1.0.0b1) (3.7.1)\n",
      "Requirement already satisfied: requests>=2.21.0 in /Users/clemlesne/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from azure-core<2.0.0,>=1.28.0->azure-ai-documentintelligence==1.0.0b1) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /Users/clemlesne/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from azure-core<2.0.0,>=1.28.0->azure-ai-documentintelligence==1.0.0b1) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Users/clemlesne/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from azure-core<2.0.0,>=1.28.0->azure-ai-documentintelligence==1.0.0b1) (4.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/clemlesne/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from anyio<5.0,>=3.0->azure-core<2.0.0,>=1.28.0->azure-ai-documentintelligence==1.0.0b1) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/clemlesne/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from anyio<5.0,>=3.0->azure-core<2.0.0,>=1.28.0->azure-ai-documentintelligence==1.0.0b1) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/clemlesne/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-ai-documentintelligence==1.0.0b1) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/clemlesne/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-ai-documentintelligence==1.0.0b1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/clemlesne/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-ai-documentintelligence==1.0.0b1) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install azure-ai-documentintelligence==1.0.0b1 azure-search-documents==11.4.0 unidecode==1.3.8 nltk==3.8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus, install NLTK data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/clemlesne/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "\n",
    "download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, initialize the clients to Document Intelligence and AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.documentintelligence.aio import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeResult\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "from azure.search.documents import SearchClient\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textwrap import dedent\n",
    "from typing import Dict, Tuple\n",
    "from unidecode import unidecode\n",
    "import asyncio\n",
    "import glob\n",
    "import re\n",
    "\n",
    "source = \"car-peugeot-206.pdf\"\n",
    "\n",
    "doc_endpoint = \"https://claim-ai.cognitiveservices.azure.com\"\n",
    "doc_credential = AzureKeyCredential(\"xxx\")\n",
    "doc_client = DocumentIntelligenceClient(\n",
    "  endpoint=doc_endpoint,\n",
    "  credential=doc_credential,\n",
    ")\n",
    "\n",
    "search_endpoint = \"https://claim-ai.search.windows.net\"\n",
    "search_credential = AzureKeyCredential(\"xxx\")\n",
    "search_client = SearchClient(\n",
    "  endpoint=search_endpoint,\n",
    "  index_name=\"trainings\",\n",
    "  credential=search_credential,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, transform PDF in Markdown text. We are using Document Intelligence for that.\n",
    "\n",
    "Be warned that this step can take a few minutes, depending on the size of the PDF. From a minute for a few pages to 20 minutes for a 1000 pages PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset/20210300_CG_GH_3350-56221.pdf\n",
      "Starting dataset/CYBER_SECURITE_3350-93501-032023-Web.pdf\n",
      "Starting dataset/DISPOSITIONS GENERALES GROUPAMA COHESION.pdf\n",
      "Starting dataset/CG-groupama-habitation 56221-062018.pdf\n",
      "Starting dataset/conditions_generales_multirisque_climatique_2022.pdf\n",
      "Starting dataset/Code des assurances 2024-1.pdf\n",
      "Starting dataset/conditions_generales_groupama.pdf\n",
      "Starting dataset/Conditions GÃ©nÃ©rales Chasse-01.pdf\n",
      "Starting dataset/Groupama-URD2021-FR-ecobook.pdf\n",
      "Starting dataset/Code des assurances 2024-2.pdf\n",
      "Starting dataset/Code des assurances 2024-3.pdf\n",
      "Starting dataset/CG MRH GROUPAMA.pdf\n",
      "Starting dataset/Groupama-Offre.pdf\n",
      "Starting dataset/conditions-generales-assurance-auto-groupama.pdf\n",
      "Starting dataset/GROUPAMA_Document-dEnregistrement-Universel_2019.pdf\n",
      "Starting dataset/Groupama_Sante_Prevoyance_CG-2.pdf\n",
      "Waiting for results...\n",
      "Ended dataset/Groupama-Offre.pdf\n",
      "Ended dataset/CYBER_SECURITE_3350-93501-032023-Web.pdf\n",
      "Ended dataset/Conditions GÃ©nÃ©rales Chasse-01.pdf\n",
      "Ended dataset/DISPOSITIONS GENERALES GROUPAMA COHESION.pdf\n",
      "Ended dataset/conditions_generales_groupama.pdf\n",
      "Ended dataset/CG MRH GROUPAMA.pdf\n",
      "Ended dataset/Groupama_Sante_Prevoyance_CG-2.pdf\n",
      "Ended dataset/conditions_generales_multirisque_climatique_2022.pdf\n",
      "Ended dataset/conditions-generales-assurance-auto-groupama.pdf\n",
      "Ended dataset/CG-groupama-habitation 56221-062018.pdf\n",
      "Ended dataset/20210300_CG_GH_3350-56221.pdf\n",
      "Ended dataset/Code des assurances 2024-2.pdf\n",
      "Ended dataset/Code des assurances 2024-3.pdf\n",
      "Ended dataset/Code des assurances 2024-1.pdf\n"
     ]
    }
   ],
   "source": [
    "async def pdf_to_markdown(source: str) -> Tuple[str, AnalyzeResult]:\n",
    "    with open(source, \"rb\") as file:\n",
    "        doc_poller = await doc_client.begin_analyze_document(\n",
    "            analyze_request=file,\n",
    "            content_type=\"application/octet-stream\",\n",
    "            locale=\"fr-FR\",  # We only have French documents in this dataset\n",
    "            model_id=\"prebuilt-layout\",\n",
    "            output_content_format=\"markdown\",\n",
    "        )\n",
    "        doc_result = await doc_poller.result()\n",
    "        return source, doc_result\n",
    "\n",
    "\n",
    "doc_results: Dict[str, AnalyzeResult] = {}\n",
    "doc_tasks = []\n",
    "\n",
    "for source in glob.glob(\"dataset/*.pdf\"):\n",
    "    print(f\"Starting {source}\")\n",
    "    doc_tasks.append(asyncio.create_task(pdf_to_markdown(source)))\n",
    "\n",
    "print(\"Waiting for results...\")\n",
    "for doc_task in asyncio.as_completed(doc_tasks):\n",
    "    try:\n",
    "        source, doc_result = await doc_task\n",
    "        print(f\"Ended {source}\")\n",
    "        doc_results[source] = doc_result\n",
    "    except HttpResponseError as e:\n",
    "        print(f\"Failed {source}: {e}\")\n",
    "\n",
    "# doc_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the Markdown text into smaller blocks, we'll call chuncks. Each block is divided, at a minimum, by level 4 headers. Each block content is minified with a stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 8768 chunks\n"
     ]
    }
   ],
   "source": [
    "lang = \"french\"\n",
    "stemmer = SnowballStemmer(lang)\n",
    "\n",
    "\n",
    "def compress(text: str) -> str:\n",
    "    tokenized = word_tokenize(text, language=lang)\n",
    "    tokens = [stemmer.stem(token) for token in tokenized]\n",
    "    prompt = \" \".join(tokens)\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def data(content: str, source_uri: str, title: str, iterator: int) -> dict[str, str]:\n",
    "    return {\n",
    "        \"content\": content,\n",
    "        \"id\": f\"{'_'.join(re.sub('[^a-z0-9]', ' ', unidecode(source_uri).lower()).split())}-{iterator}\",  # Use deterministic ID to avoid duplicates after a new run\n",
    "        \"source_uri\": unidecode(source_uri).lower(),\n",
    "        \"title\": ' '.join(re.sub('[^a-z0-9]', ' ', unidecode(title).lower()).split()),  # Remove all special characters\n",
    "    }\n",
    "\n",
    "\n",
    "documents = []\n",
    "iterator = 0\n",
    "\n",
    "for source, doc_result in doc_results.items():\n",
    "  for h1_section in (doc_result.content or \"\").split(\"\\n#\"):\n",
    "    h1_title = h1_section.split(\"\\n\")[0].strip()\n",
    "    for h2_section in h1_section.split(\"\\n##\"):\n",
    "      h2_title = h2_section.split(\"\\n\")[0].strip()\n",
    "      for h3_section in h2_section.split(\"\\n###\"):\n",
    "        h3_title = h3_section.split(\"\\n\")[0].strip()\n",
    "        for h4_section in h3_section.split(\"\\n###\"):\n",
    "          h4_title = h4_section.split(\"\\n\")[0].strip()\n",
    "          h4_content = compress(\" \".join(h4_section.split(\"\\n\")[1:]))\n",
    "          content = dedent(f\"\"\"\n",
    "            # {h1_title}\n",
    "            ## {h2_title}\n",
    "            ### {h3_title}\n",
    "            #### {h4_title}\n",
    "            {h4_content}\n",
    "          \"\"\")\n",
    "          documents.append(data(content, source, h4_title, iterator))\n",
    "          iterator += 1\n",
    "\n",
    "        else: # No H4\n",
    "          h3_content = compress(\" \".join(h3_section.split(\"\\n\")[1:]))\n",
    "          content = dedent(f\"\"\"\n",
    "            # {h1_title}\n",
    "            ## {h2_title}\n",
    "            ### {h3_title}\n",
    "            {h3_content}\n",
    "          \"\"\")\n",
    "          documents.append(data(content, source, h3_title, iterator))\n",
    "          iterator += 1\n",
    "\n",
    "      else:  # No H3\n",
    "        h2_content = compress(\" \".join(h2_section.split(\"\\n\")[1:]))\n",
    "        content = dedent(f\"\"\"\n",
    "          # {h1_title}\n",
    "          ## {h2_title}\n",
    "          {h2_content}\n",
    "        \"\"\")\n",
    "        documents.append(data(content, source, h2_title, iterator))\n",
    "        iterator += 1\n",
    "\n",
    "    else:  # No H2\n",
    "      h1_content = compress(\" \".join(h1_section.split(\"\\n\")[1:]))\n",
    "      content = dedent(f\"\"\"\n",
    "        # {h1_title}\n",
    "        {h1_content}\n",
    "      \"\"\")\n",
    "      documents.append(data(content, source, h1_title, iterator))\n",
    "      iterator += 1\n",
    "\n",
    "print(f\"Created {len(documents)} chunks\")\n",
    "# documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, upload the chuncks to AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading 8768 documents to Azure Search\n",
      "There are 13628 documents in the index\n"
     ]
    }
   ],
   "source": [
    "print(f\"Uploading {len(documents)} documents to Azure Search\")\n",
    "search_client.merge_or_upload_documents(documents)\n",
    "print(f\"There are {search_client.get_document_count()} documents in the index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! ðŸ˜Ž"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
