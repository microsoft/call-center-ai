from deepeval import assert_test
from deepeval.metrics import (
    AnswerRelevancyMetric,
    BiasMetric,
    ContextualRelevancyMetric,
    ToxicityMetric,
)
from deepeval.models.gpt_model import GPTModel
from deepeval.test_case import LLMTestCase
from helpers.call_events import (
    on_call_connected,
    on_call_disconnected,
    on_end_call,
    on_ivr_recognized,
    on_play_completed,
    on_speech_recognized,
)
from datetime import datetime
from deepeval.metrics import BaseMetric
from helpers.logging import logger
from models.call import CallStateModel
from models.reminder import ReminderModel
from models.training import TrainingModel
from pydantic import TypeAdapter
from pytest import assume
from tests.conftest import CallAutomationClientMock, with_conversations
from typing import Optional
import asyncio
import json
import pytest
import re


class ClaimRelevancyMetric(BaseMetric):
    call: CallStateModel
    model: GPTModel
    threshold: float

    def __init__(
        self,
        call: CallStateModel,
        model: GPTModel,
        threshold: float = 0.5,
    ):
        super().__init__()
        self.call = call
        self.model = model
        self.threshold = threshold

    async def a_measure(
        self,
        test_case: LLMTestCase,
        *args,
        **kwargs,
    ) -> float:
        assert test_case.input
        # Extract claim data
        extracts = await self._extract_claim_theory(test_case.input)
        logger.info(f"Extracted claim data: {extracts}")
        # Measure each claim in parallel
        scores = await asyncio.gather(
            *[
                self._score_data(
                    key=key,
                    throry=value,
                    real=str(self.call.claim.get(key, None)),
                )
                for key, value in extracts.items()
            ]
        )
        logger.info(f"Claim scores: {scores}")
        # Score is the average
        self.score = sum(scores) / len(scores) if len(extracts) > 0 else 1
        # Test against the threshold
        self.success = self.score >= self.threshold
        return self.score

    async def _score_data(self, key: str, throry: str, real: Optional[str]) -> float:
        res, _ = await self.model.a_generate(
            f"""
            Assistant is a data analyst expert with 20 years of experience.

            # Context
            Connversation come from a call center.

            # Objective
            Score the relevancy of a theoritical data against the real one.

            # Rules
            - A high score means the real data is correct
            - A low score means the real data is not relevant and lacks details against the theoritical data
            - Respond only with the score, nothing else
            - The score should be between 0 and 1

            # Data
            Name: {key}
            Throry: {throry or "N/A"}
            Real: {real or "N/A"}

            # Response format
            score, float between 0.0 and 1.0

            ## Example 1
            Name: age
            Throry: 25 years old, born in 1996
            Real: 25
            Assistant: 1

            ## Example 2
            Name: street_address
            Throry: 123 Main St, New York, NY 10001
            Real: United States
            Assistant: 0.2

            ## Example 3
            Name: phone
            Throry: 123-456-7890
            Real: +11234567890
            Assistant: 1.0

            ## Example 4
            Name: email
            Throry: john.doe@gmail.com
            Real: marie.rac@yahoo.fr
            Assistant: 0.0

            ## Example 5
            Name: incident_location
            Throry: Near the Eiffel Tower, Paris, France
            Real: Eiffel Tower
            Assistant: 0.6

            ## Example 6
            Name: incident_date
            Throry: 2021-01-01
            Real: N/A
            Assistant: 0.0
        """
        )
        try:
            score = float(res)
        except ValueError:
            group = re.search(r"\d+\.\d+", res)
            if group:
                return float(group.group())
            raise ValueError(f"LLM response is not a number: {res}")
        return score

    async def _extract_claim_theory(self, conversation: str) -> dict[str, str]:
        res, _ = await self.model.a_generate(
            f"""
            Assistant is a data analyst expert with 20 years of experience.

            # Context
            Conversation is coming from a call centre. Today is {datetime.now(self.call.tz()).strftime("%Y-%m-%d %H:%M (%Z)")}.

            # Objective
            Extract fields from a conversation. The respond will be a JSON object with the key-value pairs.

            # Rules
            - All data should be extracted
            - Respond only with the JSON object, nothing else
            - If there is no data to populate a field, do not include it
            - Limit fields to the ones listed
            - Values should be detailed, if data exists

            # Fields
            {", ".join([f"{field.name} ({field.type.value})" for field in self.call.initiate.claim])}

            # Conversation
            {conversation}

            # Response format in JSON
            [
                {{
                    "key": "[key]",
                    "value": "[value]"
                }}
            ]

            ## Example 1
            Conversation: I am 25 years old and I was born in 1996.
            Fields: age (text), birth_date (datetime), car_model (text)
            Assistant: [{{"key": "age", "value": "25 years old"}}, {{"key": "birth_date", "value": "1996-05-07"}}]

            ## Example 2
            Conversation: I live at 123 Main St, New York, NY 10001. My car is a Ford F-150 2021.
            Fields: street_address (text), car_model (text), phone (phone_number)
            Assistant: [{{"key": "street_address", "value": "123 Main St, New York, NY 10001"}}, {{"key": "car_model", "value": "Ford F-150 2021"}}]

            ## Example 3
            Conversation: zsssk zsssk
            Fields: incident_location (text), incident_date (datetime)
            Assistant: []

            ## Example 4
            Conversation: My name is John Doe.
            Fields: car_model (text)
            Assistant: []
        """
        )
        res = res.strip().strip("```json\n").strip("\n```").strip()
        extracts: list[dict[str, str]] = json.loads(res)
        return {
            extract["key"]: extract["value"]
            for extract in extracts
            if "value" in extract
            and "key" in extract
            and any(
                claim_field.name == extract["key"]
                for claim_field in self.call.initiate.claim
            )
        }

    def is_successful(self) -> bool:
        return self.success or False

    @property
    def __name__(self):
        return "Claim Relevancy"


@with_conversations
@pytest.mark.asyncio(scope="session")
async def test_llm(
    call: CallStateModel,
    claim_tests_excl: list[str],
    deepeval_model: GPTModel,
    expected_output: str,
    inputs: list[str],
    lang: str,
) -> None:
    """
    Test the LLM with a mocked conversation against the expected output.

    Steps:
    1. Run application with mocked inputs
    2. Combine all outputs
    3. Test claim data exists
    4. Test LLM metrics
    """

    def _play_media_callback(text: str) -> None:
        nonlocal actual_output
        actual_output += f" {text}"

    actual_output = ""

    # Mock client
    automation_client = CallAutomationClientMock(
        hang_up_callback=lambda: None,
        play_media_callback=_play_media_callback,
        transfer_callback=lambda: None,
    )
    call_client = automation_client.get_call_connection()

    # Mock call
    call.lang = lang

    async def _post_callback(_call: CallStateModel) -> None:
        await on_end_call(call=_call)

    async def _trainings_callback(_call: CallStateModel) -> None:
        await _call.trainings(cache_only=False)

    # Connect call
    await on_call_connected(
        call=call,
        client=automation_client,
    )

    # First IVR
    await on_ivr_recognized(
        call=call,
        client=automation_client,
        label=call.lang.short_code,
        post_callback=_post_callback,
        trainings_callback=_trainings_callback,
    )

    # Simulate conversation with speech recognition
    for input in inputs:
        # Respond
        await on_speech_recognized(
            call=call,
            client=automation_client,
            post_callback=_post_callback,
            text=input,
            trainings_callback=_trainings_callback,
        )
        # Receip
        await on_play_completed(
            call=call,
            client=automation_client,
            contexts=call_client.last_contexts,
            post_callback=_post_callback,
        )
        # Reset contexts
        call_client.last_contexts.clear()

    # Disconnect call
    await on_call_disconnected(
        call=call,
        client=automation_client,
        post_callback=_post_callback,
    )

    # Remove newlines for log comparison
    actual_output = _remove_newlines(actual_output)
    full_input = _remove_newlines(" ".join(inputs))

    # Log for dev review
    logger.info(f"actual_output: {actual_output}")
    logger.info(f"claim: {call.claim}")
    logger.info(f"full_input: {full_input}")

    # Configure LLM tests
    test_case = LLMTestCase(
        actual_output=actual_output,
        expected_output=expected_output,
        input=full_input,
        retrieval_context=[
            json.dumps(call.claim),
            TypeAdapter(list[ReminderModel]).dump_json(call.reminders).decode(),
            TypeAdapter(list[TrainingModel]).dump_json(await call.trainings()).decode(),
        ],
    )

    assume(call.next, "No next action found")
    assume(call.synthesis, "No synthesis found")

    # Define LLM metrics
    llm_metrics = [
        BiasMetric(threshold=1, model=deepeval_model),  # Gender, age, ethnicity
        ClaimRelevancyMetric(
            call=call,
            model=deepeval_model,
            threshold=0.5,
        ),  # Claim data
        ToxicityMetric(threshold=1, model=deepeval_model),  # Hate speech, insults
    ]  # Include those by default

    if not any(
        field == "answer_relevancy" for field in claim_tests_excl
    ):  # Test respond relevancy from questions
        llm_metrics.append(AnswerRelevancyMetric(threshold=0.5, model=deepeval_model))
    if not any(
        field == "contextual_relevancy" for field in claim_tests_excl
    ):  # Test respond relevancy from context
        llm_metrics.append(
            ContextualRelevancyMetric(threshold=0.25, model=deepeval_model)
        )

    # Execute LLM tests
    assert_test(test_case, llm_metrics)


def _remove_newlines(text: str) -> str:
    """
    Remove newlines from a string and return it as a single line.
    """
    return " ".join([line.strip() for line in text.splitlines()])
